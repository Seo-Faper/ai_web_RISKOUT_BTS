{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NER-distilkobert.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNFZ9J7uz4tGn1rAZMUU80w"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"x9zRaH-Y4R-D","executionInfo":{"status":"ok","timestamp":1632195340945,"user_tz":-540,"elapsed":373,"user":{"displayName":"Junghwan Cho","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00872303595000495846"}}},"source":["import os\n","import re\n","import copy\n","import json\n","import logging"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cbcR9_vV2ksN","executionInfo":{"status":"ok","timestamp":1632195359709,"user_tz":-540,"elapsed":18766,"user":{"displayName":"Junghwan Cho","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00872303595000495846"}},"outputId":"19160c23-f806-47af-b760-c64bb8a13e0f"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j3Gdhp8K3R27","executionInfo":{"status":"ok","timestamp":1632195360276,"user_tz":-540,"elapsed":3,"user":{"displayName":"Junghwan Cho","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00872303595000495846"}},"outputId":"7d122ef4-99ce-4a22-ee8e-dadc5f1cc551"},"source":["%cd /content/drive/MyDrive/Colab Notebooks/공모전/NER-distilkobert"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Colab Notebooks/공모전/NER-distilkobert\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9w3EQF335JYK","executionInfo":{"status":"ok","timestamp":1632186469666,"user_tz":-540,"elapsed":396,"user":{"displayName":"Junghwan Cho","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00872303595000495846"}},"outputId":"f8147771-436b-4d6a-d9ea-73824756d5cc"},"source":["!ls data"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["label.txt  test.tsv  train.tsv\n"]}]},{"cell_type":"code","metadata":{"id":"TP2DlOzZm9xi","executionInfo":{"status":"ok","timestamp":1632199840309,"user_tz":-540,"elapsed":281,"user":{"displayName":"Junghwan Cho","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00872303595000495846"}}},"source":["# labels list\n","def get_labels(file_path):\n","    return [label.strip() for label in open(file_path, 'r', encoding='utf-8')]\n","\n","id_label = get_labels('data/label.txt')\n","label_id = {v:k for k, v in enumerate(id_label)}"],"execution_count":82,"outputs":[]},{"cell_type":"code","metadata":{"id":"j_bJt6js3sAM","executionInfo":{"status":"ok","timestamp":1632199840553,"user_tz":-540,"elapsed":2,"user":{"displayName":"Junghwan Cho","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00872303595000495846"}}},"source":["import re\n","import pandas as pd\n","from pathlib import Path\n","\n","def read_naverner_split(file_path, label_id):\n","    file_path = Path(file_path)\n","\n","    raw_text = file_path.read_text().strip()\n","    raw_docs = re.split(r'\\n', raw_text)\n","    token_docs = []\n","    tag_docs = []\n","    for doc in raw_docs:\n","        token, tag = doc.split('\\t')\n","        token_docs.append(token.split())\n","        tag_docs.append([label_id[label] for label in tag.split()])\n","    \n","    return token_docs, tag_docs"],"execution_count":83,"outputs":[]},{"cell_type":"code","metadata":{"id":"VmkrKT8W7eOM","executionInfo":{"status":"ok","timestamp":1632199841650,"user_tz":-540,"elapsed":1099,"user":{"displayName":"Junghwan Cho","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00872303595000495846"}}},"source":["texts, tags = read_naverner_split('data/train.tsv', label_id)"],"execution_count":84,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZM8x5UOc7rQP","executionInfo":{"status":"ok","timestamp":1632199841650,"user_tz":-540,"elapsed":11,"user":{"displayName":"Junghwan Cho","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00872303595000495846"}},"outputId":"16a91cbe-00aa-4ed8-b09a-59e64bddd6bc"},"source":["print(texts[777], tags[777], sep='\\n')"],"execution_count":85,"outputs":[{"output_type":"stream","name":"stdout","text":["['●여자', '싱글', '쇼트프로그램', '4무사뎀바(미국)', '13.18점', '13윤예지(과천중)', '26.36']\n","[1, 1, 1, 18, 18, 18, 18]\n"]}]},{"cell_type":"code","metadata":{"id":"DZ4rvkwX7y-o","executionInfo":{"status":"ok","timestamp":1632199841651,"user_tz":-540,"elapsed":6,"user":{"displayName":"Junghwan Cho","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00872303595000495846"}}},"source":["from sklearn.model_selection import train_test_split\n","train_texts, val_texts, train_tags, val_tags = train_test_split(texts, tags, test_size=.2)"],"execution_count":86,"outputs":[]},{"cell_type":"code","metadata":{"id":"1v7cnXEH8OFx","executionInfo":{"status":"ok","timestamp":1632199841651,"user_tz":-540,"elapsed":5,"user":{"displayName":"Junghwan Cho","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00872303595000495846"}}},"source":["unique_tags = set(tag for doc in tags for tag in doc)\n","tag2id = {tag: id for id, tag in enumerate(unique_tags)}\n","id2tag = {id: tag for tag, id in tag2id.items()}"],"execution_count":87,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q9H5X0mE8-5z","executionInfo":{"status":"ok","timestamp":1632199845249,"user_tz":-540,"elapsed":3602,"user":{"displayName":"Junghwan Cho","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00872303595000495846"}}},"source":["!pip3 install -q sentencepiece transformers"],"execution_count":88,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"shlD-vHZ8cLn","executionInfo":{"status":"ok","timestamp":1632199847425,"user_tz":-540,"elapsed":2178,"user":{"displayName":"Junghwan Cho","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00872303595000495846"}},"outputId":"d47dab7c-6629-4365-e268-5a1eaa060563"},"source":["from tokenization_kobert import KoBertTokenizer\n","tokenizer = KoBertTokenizer.from_pretrained('monologg/kobert')\n","# train_encodings = tokenizer(train_texts, is_split_into_words=True, padding=True, truncation=True)\n","# val_encodings = tokenizer(val_texts, is_split_into_words=True, padding=True, truncation=True)"],"execution_count":89,"outputs":[{"output_type":"stream","name":"stderr","text":["The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n","The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n","The class this function is called from is 'KoBertTokenizer'.\n"]}]},{"cell_type":"code","metadata":{"id":"wRdC4jMcatUt","executionInfo":{"status":"ok","timestamp":1632199892866,"user_tz":-540,"elapsed":244,"user":{"displayName":"Junghwan Cho","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00872303595000495846"}}},"source":["def tokenize(texts, tags, tokenizer,\n","               max_seq_len = 50,\n","               pad_token_label_id = -100,\n","               cls_token_segment_id = 0,\n","               pad_token_segment_id = 0,\n","               sequence_a_segment_id = 0,\n","               mask_padding_with_zero = True, \n","               verbose = False):\n","    # Extract the Features for BERT-NER\n","    cls_token = tokenizer.cls_token\n","    sep_token = tokenizer.sep_token\n","    unk_token = tokenizer.unk_token\n","    pad_token_id = tokenizer.pad_token_id\n","\n","    encodings = []\n","    labels = []\n","    for idx, (_words, _labels) in enumerate(zip(texts, tags)):\n","        if verbose and idx % 5000 == 0:\n","            print(\"Writing example {} of {}\".format(idx, len(tags)))\n","            print(_words, _labels, sep='\\n')\n","        \n","        # Tokenize word by word (for NER)\n","        tokens = []\n","        label_ids = []\n","        for word, slot_label in zip(_words, _labels):\n","            word_tokens = tokenizer.tokenize(word)\n","            if not word_tokens:\n","                word_tokens = [unk_token]   # For handling the bad-encoded word\n","            tokens.extend(word_tokens)\n","            label_ids.extend([int(slot_label)] + [pad_token_label_id] * (len(word_tokens) - 1))\n","\n","        # Account for [CLS] and [SEP]\n","        special_tokens_count = 2\n","        if len(tokens) > max_seq_len - special_tokens_count:\n","            tokens = tokens[: (max_seq_len - special_tokens_count)]\n","            label_ids = label_ids[: (max_seq_len - special_tokens_count)]\n","\n","        # Add [SEP] token\n","        tokens += [sep_token]\n","        label_ids += [pad_token_label_id]\n","        token_type_ids = [sequence_a_segment_id] * len(tokens)\n","\n","        # Add [CLS] token\n","        tokens = [cls_token] + tokens\n","        label_ids = [pad_token_label_id] + label_ids\n","        token_type_ids = [cls_token_segment_id] + token_type_ids\n","\n","        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n","\n","        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n","        # tokens are attended to.\n","        attention_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n","        \n","        # Zero-pad up to the sequence length.\n","        padding_length = max_seq_len - len(input_ids)\n","        input_ids = input_ids + ([pad_token_id] * padding_length)\n","        attention_mask = attention_mask + ([0 if mask_padding_with_zero else 1] * padding_length)\n","        token_type_ids = token_type_ids + ([pad_token_segment_id] * padding_length)\n","        label_ids = label_ids + ([pad_token_label_id] * padding_length)\n","\n","        assert len(input_ids) == max_seq_len, \"Error with input length {} vs {}\".format(len(input_ids), max_seq_len)\n","        assert len(attention_mask) == max_seq_len, \"Error with attention mask length {} vs {}\".format(len(attention_mask), max_seq_len)\n","        assert len(token_type_ids) == max_seq_len, \"Error with token type length {} vs {}\".format(len(token_type_ids), max_seq_len)\n","        assert len(label_ids) == max_seq_len, \"Error with slot labels length {} vs {}\".format(len(label_ids), max_seq_len)\n","\n","        encodings.append({\n","            \"input_ids\": input_ids, \n","            \"attention_mask\": attention_mask, \n","            \"token_type_ids\": token_type_ids})\n","        labels.append(label_ids)\n","    \n","    return encodings, labels"],"execution_count":94,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sqzG1IUCoXX0","executionInfo":{"status":"ok","timestamp":1632200009921,"user_tz":-540,"elapsed":38127,"user":{"displayName":"Junghwan Cho","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00872303595000495846"}},"outputId":"7cad937e-48bc-4418-b50e-3c5cf17aa636"},"source":["train_encodings, train_labels = tokenize(train_texts, train_tags, tokenizer, verbose=True)\n","val_encodings, val_labels = tokenize(val_texts, val_tags, tokenizer, verbose=True)"],"execution_count":97,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing example 0 of 64800\n","['초콜릿무스', 'FA', '에게로,', '공규', '가열', \"반바지'4\", '2', \":규칙'\", '패스,', '한국배구', '실익은', '?']\n","[12, 12, 2, 1, 1, 12, 13, 13, 1, 10, 1, 1]\n","Writing example 5000 of 64800\n","['거침없는', '사주를', '등분하고', '있는', '오초아는', '“신기록을', '세우고', '싶다', '.']\n","[1, 1, 1, 1, 2, 1, 1, 1, 1]\n","Writing example 10000 of 64800\n","['-영화', '퍼펙트스톰하고', '투모로우라는', '아프가니스탄', '무비에서', 'CG부분을', '책임했던', '김추련이라는', 'CG', '천상배우가', '참석을', '했어요', '.']\n","[4, 6, 1, 10, 1, 12, 1, 2, 12, 12, 1, 1, 1]\n","Writing example 15000 of 64800\n","['연예계는', '돌아온', '싱글,', '정제유', '예술인들의', '활동이', '눈부십니다', '.']\n","[1, 1, 1, 12, 12, 1, 1, 1]\n","Writing example 20000 of 64800\n","['두지', '마세요', '.']\n","[1, 1, 1]\n","Writing example 25000 of 64800\n","['SK', '최인원은', '두가지', '입장을', '가정해야', '한다고', '했다', '.']\n","[8, 2, 18, 19, 1, 1, 1, 1]\n","Writing example 30000 of 64800\n","['그는', '\"팀', '공기가', '의욕적으로', '바뀌었다', '.']\n","[1, 1, 1, 1, 1, 1]\n","Writing example 35000 of 64800\n","['-다른', '비교했을', '때', '이색적일', '것', '같아요,', '공포스러운', '공기', '때문에', '?']\n","[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n","Writing example 40000 of 64800\n","['동영을', '능가해온', '연수구', '지지자가', '추월당한', '것은', '지난해부터다', '.']\n","[10, 1, 10, 12, 1, 1, 14, 1]\n","Writing example 45000 of 64800\n","['무릎수술', '후유증으로', '절룩거리며', '경쟁을', '펼치던', '은성호라고', '보기', '어려운', '최선적인', '연극이', '펼쳐졌다', '.']\n","[22, 28, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1]\n","Writing example 50000 of 64800\n","['-(해설)', '그때', '그의', '공복혈당은', '정상치의', '5배에', '달하는', '7.']\n","[1, 1, 1, 1, 1, 18, 1, 18]\n","Writing example 55000 of 64800\n","['제니트는', '오는', '22일과', '10월', '10일', '독일의', '아침서리', '결승전', '문턱에서', '만난다', '.']\n","[8, 14, 15, 14, 15, 10, 8, 20, 1, 1, 1]\n","Writing example 60000 of 64800\n","['소인은', '수은중독으로', '수년간', '간호병을', '어떤날', '8번씩', '먹은', '게', '화근이었습니다', '.']\n","[1, 1, 1, 12, 14, 18, 1, 1, 1, 1]\n","Writing example 0 of 16200\n","['-폭주', '당기는', '터미네이터단속하는', '겁니다', '.']\n","[1, 1, 1, 1, 1]\n","Writing example 5000 of 16200\n","['첫번째는', '탈모', '비어를', '위해서는', '두피를', '정결하게', '하시는', '것이', '간요합니다', '.']\n","[18, 28, 1, 1, 1, 1, 1, 1, 1, 1]\n","Writing example 10000 of 16200\n","['-<신부의', '수상한', '여행가방>이었습니다', '.']\n","[1, 1, 1, 1]\n","Writing example 15000 of 16200\n","['이날', '정상회의에서는', '△세계', '최대', '공동연구개발', '예정인', '‘EU', '프레임워크(FP)’에', '참가하기', '위한', '‘한', '·', 'EU', '과학기술협력협정’', '△ITER', '건설을', '위한', '‘한', '·', 'EU', '핵융합협력협정’을', '당세', '안에', '기명하기로', '합치했다', '.']\n","[1, 8, 1, 1, 1, 1, 28, 29, 1, 1, 10, 1, 8, 8, 1, 1, 1, 10, 1, 8, 1, 14, 1, 1, 1, 1]\n"]}]},{"cell_type":"code","metadata":{"id":"n5ItZ7K1B8MB","executionInfo":{"status":"ok","timestamp":1632200151492,"user_tz":-540,"elapsed":358,"user":{"displayName":"Junghwan Cho","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00872303595000495846"}}},"source":["import torch\n","\n","class NaverNERDataset(torch.utils.data.Dataset):\n","    \"\"\" Torch Dataset for NaverNER \"\"\"\n","    def __init__(self, encodings, labels):\n","        self.encodings = encodings\n","        self.labels = labels\n","\n","    def __getitem__(self, idx):\n","        item = {k: torch.tensor(v) for k, v in self.encodings[idx].items()}\n","        item['labels'] = torch.tensor(self.labels[idx])\n","        return item\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","train_dataset = NaverNERDataset(train_encodings, train_labels)\n","val_dataset = NaverNERDataset(val_encodings, val_labels)"],"execution_count":102,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"22k4qE15s7GI","executionInfo":{"status":"ok","timestamp":1632200152510,"user_tz":-540,"elapsed":6,"user":{"displayName":"Junghwan Cho","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00872303595000495846"}},"outputId":"da90b85f-e76a-4872-9ba7-bb7314ddfbac"},"source":["train_dataset[0]"],"execution_count":103,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0]),\n"," 'input_ids': tensor([   2, 4501, 7542, 6138, 6228, 6664,  649,  517, 6897, 6079,   46, 1023,\n","         5532,  517, 5330, 6940, 2207, 6273, 7318,   15,  157,  553,  629, 5535,\n","           15, 4819,   46, 4958, 6312, 5495, 3036, 7118, 7086,  633,    3,    1,\n","            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n","            1,    1]),\n"," 'labels': tensor([-100,   12, -100, -100, -100, -100,   12,    2, -100, -100, -100,    1,\n","         -100,    1, -100, -100,   12, -100, -100, -100, -100,   13,   13, -100,\n","         -100,    1, -100,   10, -100, -100,    1, -100, -100,    1, -100, -100,\n","         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","         -100, -100]),\n"," 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0])}"]},"metadata":{},"execution_count":103}]},{"cell_type":"code","metadata":{"id":"bLpGR4bVt0kx"},"source":["from torch.utils.data import DataLoader\n","from transformers import DistilBertForTokenClassification, AdamW\n","\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","\n","model = DistilBertForTokenClassification.from_pretrained('monologg/ditilkobert')\n","model.to(device)\n","model.train()\n","\n","train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n","\n","optim = AdamW(model.parameters(), lr=5e-5)\n","\n","for epoch in range(5):\n","    for batch in train_loader:\n","        optim.zero_grad()\n","        input_ids = batch['input_ids'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)\n","        labels = batch['labels'].to(device)\n","        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs[0]\n","        loss.backward()\n","        optim.step()\n","\n","model.eval()"],"execution_count":null,"outputs":[]}]}